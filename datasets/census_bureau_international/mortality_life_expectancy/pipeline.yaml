# Copyright 2021 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

---
resources:

  - type: bigquery_table

    table_id: mortality_life_expectancy

    description: "Mortality_Life_Expectancy Dataset"
dag:
  airflow_version: 1
  initialize:
    dag_id: mortality_life_expectancy
    default_args:
      owner: "Google"
      depends_on_past: False
      start_date: '2021-03-01'
    max_active_runs: 1
    schedule_interval: "@daily"
    catchup: False
    default_view: graph

  tasks:

    - operator: "KubernetesPodOperator"

      description: "Run CSV transform within kubernetes pod"

      args:

        task_id: "mortality_life_expectancy_transform_csv"

        startup_timeout_seconds: 600

        name: "mortality_life_expectancy

        namespace: "default"

         affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
                - matchExpressions:
                    - key: cloud.google.com/gke-nodepool
                      operator: In
                      values:
                        - "pool-e2-standard-4"

        image_pull_policy: "Always"

        # Docker images will be built and pushed to GCR by default whenever the `scripts/generate_dag.py` is run. To skip building and pushing images, use the optional `--skip-builds` flag.
        image: "{{ var.json.census_bureau_international.container_registry.run_csv_transform_kub }}"

        # Set the environment variables you need initialized in the container. Use these as input variables for the script your container is expected to perform.
        env_vars:
          SOURCE_URL: ["https://storage.cloud.google.com/pdp-feeds-staging/Census/idbzip/IDBext010.csv","https://storage.cloud.google.com/pdp-feeds-staging/Census/idbzip/IDBextCTYS.csv"]
          SOURCE_FILE: ["files/data1.csv", "files/data2.csv"]
          TARGET_FILE: "files/data_output.csv"
          TARGET_GCS_BUCKET: "{{ var.value.composer_bucket }}"
          PIPELINE_NAME: ["mortality_life_expectancy"]
          TARGET_GCS_PATH: "data/census_bureau_international/mortality_life_expectancy/data_output.csv"
          CSV_HEADERS: >-
            ["country_code","country_name","year","infant_mortality","infant_mortality_male","infant_mortality_female","life_expectancy","life_expectancy_male","life_expectancy_female","mortality_rate_under5","mortality_rate_under5_male","mortality_rate_under5_female","mortality_rate_1to4","mortality_rate_1to4_male","mortality_rate_1to4_female"]

        # Set resource limits for the pod here. For resource units in Kubernetes, see https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#resource-units-in-kubernetes
        resources:
          limit_memory: "4G"
          limit_cpu: "2"

    - operator: "GoogleCloudStorageToBigQueryOperator"
      description: "Task to load CSV data to a BigQuery table"

      args:
        task_id: "load_mortality_life_expectancy_to_bq"

      # The GCS bucket where the CSV file is located in.
        bucket: "{{ var.value.composer_bucket }}"

      # The GCS object path for the CSV file
        source_objects: ["data/census_bureau_international/mortality_life_expectancy/data_output.csv"]
        source_format: "CSV"
        destination_project_dataset_table: "census_bureau_international.mortality_life_expectancy"

      # Use this if your CSV file contains a header row
        skip_leading_rows: 1

      # How to write data to the table: overwrite, append, or write if empty
      # See https://cloud.google.com/bigquery/docs/reference/auditlogs/rest/Shared.Types/WriteDisposition
        write_disposition: "WRITE_TRUNCATE"

      # The BigQuery table schema based on the CSV file. For more info, see
      # https://cloud.google.com/bigquery/docs/schemas.
      # Always use snake_case and lowercase for column names, and be explicit,
      # i.e. specify modes for all columns.

        schema_fields:
          - name: "country_code"
            type: "STRING"
            mode: "REQUIRED"
          - name: "country_name"
            type: "STRING"
            mode: "NULLABLE"
          - name: "year"
            type: "INTEGER"
            mode: "NULLABLE"
          - name: "infant_mortality"
            type: "FLOAT"
            mode: "NULLABLE"
          - name: "infant_mortality_male"
            type: "FLOAT"
            mode: "NULLABLE"
          - name: "infant_mortality_female"
            type: "FLOAT"
            mode: "NULLABLE"
          - name: "life_expectancy"
            type: "FLOAT"
            mode: "NULLABLE"
          - name: "life_expectancy_male"
            type: "FLOAT"
            mode: "NULLABLE"
          - name: "life_expectancy_female"
            type: "FLOAT"
            mode: "NULLABLE"
          - name: "mortality_rate_under5"
            type: "FLOAT"
            mode: "NULLABLE"
          - name: "mortality_rate_under5_male"
            type: "FLOAT"
            mode: "NULLABLE"
          - name: "mortality_rate_under5_female"
            type: "FLOAT"
            mode: "NULLABLE"
          - name: "mortality_rate_1to4"
            type: "FLOAT"
            mode: "NULLABLE"
          - name: "mortality_rate_1to4_male"
            type: "FLOAT"
            mode: "NULLABLE"
          - name: "mortality_rate_1to4_female"
            type: "FLOAT"
            mode: "NULLABLE"


  graph_paths:
    - "mortality_life_expectancy_transform_csv >> load_mortality_life_expectancy_to_bq"
